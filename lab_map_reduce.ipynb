{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled73.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rprwqcGLHXY"
      },
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "import collections\n",
        "import multiprocessing as mp\n",
        "from functools import reduce"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyOBgMSYM0Zk"
      },
      "source": [
        "def create_file(file_name, file_size):\n",
        "  str_arr = []\n",
        "  if not os.path.isfile(file_name):\n",
        "    open(file_name, \"w\").close()\n",
        "  while os.path.getsize(file_name) / (1024 * 1024 * file_size) < 1:\n",
        "    for i in range(1024):\n",
        "      str_arr.append(''.join(random.choice(string.ascii_letters) for j in range(random.randint(32, 62))) + '\\n')\n",
        "    chunk_of_strs = ''.join(str_arr)\n",
        "    str_arr[:] = []\n",
        "\n",
        "    with open(file_name, 'a+', encoding='utf-8') as file:\n",
        "      file.write(chunk_of_strs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP5KcQwmVwH5"
      },
      "source": [
        "def mapper(chunk_start, chunk_size, file_name):\n",
        "    with open(file_name) as file:\n",
        "        file.seek(chunk_start)\n",
        "        file_content = file.read(chunk_size).splitlines()\n",
        "        counter = collections.Counter(file_content)\n",
        "        return counter\n",
        "\n",
        "\n",
        "def reducer(counter_1, counter_2):\n",
        "    return counter_1 + counter_2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-AHPBAKWefi"
      },
      "source": [
        "def split_into_chunks(file_name, chunk_size=1024 ** 2):\n",
        "    end_of_file = os.path.getsize(file_name)\n",
        "    with open(file_name, 'rb') as file:\n",
        "        chunk_end = file.tell()\n",
        "        while True:\n",
        "            chunk_start = chunk_end\n",
        "            file.seek(chunk_size, 1)\n",
        "            file.readline()\n",
        "            chunk_end = file.tell()\n",
        "\n",
        "            yield chunk_start, chunk_end - chunk_start\n",
        "\n",
        "            if chunk_end > end_of_file:\n",
        "                break"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEWoDGakLS4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4093c4d-6773-427c-dd61-1b3098662dc5"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    FILE = \"input_data.txt\"\n",
        "    create_file(FILE, 4)\n",
        "    workers_pool = mp.Pool(8)\n",
        "\n",
        "    task_queue = []\n",
        "\n",
        "    for chunk_start, chunk_size in split_into_chunks(FILE):\n",
        "        task_queue.append(workers_pool.apply_async(mapper, (chunk_start, chunk_size, FILE)))\n",
        "    result = []\n",
        "    for t in task_queue:\n",
        "        result.append(t.get())\n",
        "    workers_pool.close()\n",
        "    common_counter = reduce(reducer, result)\n",
        "    result = sorted(common_counter.items(), key=lambda pair: pair[1], reverse=True)\n",
        "\n",
        "    print(common_counter.most_common(10))\n",
        "    print(result[:10])\n",
        "    print(max(result,key=lambda item:item[1]))\n",
        "    print(min(result,key=lambda item:item[1]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('aElwFGnpYaaMTxesxTgDwzlRnjsuraMIDJvlfwEbDasiD', 1), ('kwJbKpfXkgiIDuCtyiCuMxNgowoLMNtIKlUYQLjeCmuOouIwwE', 1), ('GoleQhDsMLSnbXMPzATceSxCkWVjhUQcfBmVlhcRLWHiGNXOU', 1), ('pLizilWqCOtfoTBIIAjqLjwzJopVTTprmgWqvAMSZyBGTLCmIxt', 1), ('fMhDDujxWEzvdzyAKtrMsCramTAgSFYwZgOsTULshocvqtzHc', 1), ('UHtdDtBvgXAHYMwxTCwIsosOClvMlCclhqGztjCxYKNRQWQUMWxdAlQEOuQvIL', 1), ('PhduogRXiLTPIpkihrIadAIJhdwzlcuaKyknElmKcbYmrjycBN', 1), ('FrRaMurUgekFVqLamJkZOVSOnGUvhlLusQnHZZnHMTodVqwPrtjjpAtetEWHO', 1), ('sWOXFxGGKhgcGuHPiNoQGtBgioqHYicesSRjOeosQzuod', 1), ('QubsDFcOMDsUHPXxVmBtIjohnUFjNnoooR', 1)]\n",
            "[('aElwFGnpYaaMTxesxTgDwzlRnjsuraMIDJvlfwEbDasiD', 1), ('kwJbKpfXkgiIDuCtyiCuMxNgowoLMNtIKlUYQLjeCmuOouIwwE', 1), ('GoleQhDsMLSnbXMPzATceSxCkWVjhUQcfBmVlhcRLWHiGNXOU', 1), ('pLizilWqCOtfoTBIIAjqLjwzJopVTTprmgWqvAMSZyBGTLCmIxt', 1), ('fMhDDujxWEzvdzyAKtrMsCramTAgSFYwZgOsTULshocvqtzHc', 1), ('UHtdDtBvgXAHYMwxTCwIsosOClvMlCclhqGztjCxYKNRQWQUMWxdAlQEOuQvIL', 1), ('PhduogRXiLTPIpkihrIadAIJhdwzlcuaKyknElmKcbYmrjycBN', 1), ('FrRaMurUgekFVqLamJkZOVSOnGUvhlLusQnHZZnHMTodVqwPrtjjpAtetEWHO', 1), ('sWOXFxGGKhgcGuHPiNoQGtBgioqHYicesSRjOeosQzuod', 1), ('QubsDFcOMDsUHPXxVmBtIjohnUFjNnoooR', 1)]\n",
            "('aElwFGnpYaaMTxesxTgDwzlRnjsuraMIDJvlfwEbDasiD', 1)\n",
            "('aElwFGnpYaaMTxesxTgDwzlRnjsuraMIDJvlfwEbDasiD', 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N0U-5uSLS-H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLkNTx8jLS_p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSifxvJOLTFg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}